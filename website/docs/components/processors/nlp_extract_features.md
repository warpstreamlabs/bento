---
title: nlp_extract_features
slug: nlp_extract_features
type: processor
status: beta
categories: ["Machine Learning","NLP"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Performs feature extraction using a Hugging Face ðŸ¤— NLP pipeline with an ONNX Runtime model.

Introduced in version v1.11.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
label: ""
nlp_extract_features:
  name: "" # No default (optional)
  path: /path/to/models/my_model.onnx # No default (required)
  normalization: false
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
label: ""
nlp_extract_features:
  name: "" # No default (optional)
  path: /path/to/models/my_model.onnx # No default (required)
  enable_download: false
  download_options:
    repository: KnightsAnalytics/distilbert-NER # No default (required)
    onnx_filepath: model.onnx
  normalization: false
```

</TabItem>
</Tabs>

### Feature Extraction
Feature extraction is the task of extracting features learnt in a model. This processor runs a feature extraction model against batches of text data, returning a model's multidimensional representation of said features in tensor/float64 format.
This component uses [Hugot](https://github.com/knights-analytics/hugot), a library that provides an interface for running [Open Neural Network Exchange (ONNX) models](https://onnx.ai/onnx/intro/) and transformer pipelines, with a focus on NLP tasks.

Currently, [Bento only implements](https://github.com/knights-analytics/hugot/tree/main?tab=readme-ov-file#implemented-pipelines):
	
- [featureExtraction](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.FeatureExtractionPipeline)
- [textClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextClassificationPipeline)
- [tokenClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TokenClassificationPipeline)
- [zeroShotClassification](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline)

### What is a pipeline?
From [HuggingFace docs](https://huggingface.co/docs/transformers/en/main_classes/pipelines):
> A pipeline in ðŸ¤— Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

:::warning
While, only models in [ONNX](https://onnx.ai/) format are supported, exporting existing formats to ONNX is both possible and straightforward in most standard ML libraries. For more on this, check out the [ONNX conversion docs](https://onnx.ai/onnx/intro/converters.html). 
Otherwise, check out using [HuggingFace Optimum](https://huggingface.co/docs/optimum/en/exporters/onnx/usage_guides/export_a_model) for easy model conversion.
:::


## Examples

<Tabs defaultValue="Text Embeddings" values={[
{ label: 'Text Embeddings', value: 'Text Embeddings', },
{ label: 'Document Embeddings', value: 'Document Embeddings', },
]}>

<TabItem value="Text Embeddings">

Extract normalized embeddings from text using a sentence transformer model stored locally.

```yaml
pipeline:
  processors:
    - nlp_extract_features:
        path: "onnx/model.onnx"
        normalization: true
# In: "Hello world"
# Out: [0.1234, -0.5678, 0.9012, ...] (384-dimensional vector)
```

</TabItem>
<TabItem value="Document Embeddings">

Extract raw features from documents using the all-MiniLM-L6-v2 model.

```yaml
pipeline:
  processors:
    - nlp_extract_features:
        path: "./models"
        enable_download: true
        download_options:
          repository: "sentence-transformers/all-MiniLM-L6-v2"
        normalization: false
# In: "This is a sample document for feature extraction."
# Out: [0.2341, -0.8765, 1.2345, ...] (384-dimensional vector)
```

</TabItem>
</Tabs>

## Fields

### `name`

Name of the hugot pipeline. Defaults to a random UUID if not set.


Type: `string`  

### `path`

Path to the ONNX model file, or directory containing the model. When downloading (`enable_download: true`), this becomes the destination and must be a directory.


Type: `string`  

```yml
# Examples

path: /path/to/models/my_model.onnx

path: /path/to/models/
```

### `enable_download`

When enabled, attempts to download an ONNX Runtime compatible model from HuggingFace specified in `repository`.


Type: `bool`  
Default: `false`  

### `download_options`

Options used to download a model directly from HuggingFace. Before the model is downloaded, validation occurs to ensure the remote repository contains both an`.onnx` and `tokenizers.json` file.


Type: `object`  

### `download_options.repository`

The name of the huggingface model repository.


Type: `string`  

```yml
# Examples

repository: KnightsAnalytics/distilbert-NER

repository: KnightsAnalytics/distilbert-base-uncased-finetuned-sst-2-english

repository: sentence-transformers/all-MiniLM-L6-v2
```

### `download_options.onnx_filepath`

Filepath of the ONNX model within the repository. Only needed when multiple `.onnx` files exist.


Type: `string`  
Default: `"model.onnx"`  

```yml
# Examples

onnx_filepath: onnx/model.onnx

onnx_filepath: onnx/model_quantized.onnx

onnx_filepath: onnx/model_fp16.onnx
```

### `normalization`

Whether to apply normalization in the feature extraction pipeline.


Type: `bool`  
Default: `false`  


