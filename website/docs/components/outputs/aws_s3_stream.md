---
title: aws_s3_stream
slug: aws_s3_stream
type: output
status: beta
categories: ["Services","AWS"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Streams data directly to S3 using multipart uploads with minimal memory overhead.

Introduced in version 1.0.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
output:
  label: ""
  aws_s3_stream:
    bucket: "" # No default (required)
    path: logs/${! timestamp_unix() }-${! uuid_v4() }.log # No default (required)
    max_in_flight: 64
    batching:
      count: 0
      byte_size: 0
      period: ""
      jitter: 0
      check: ""
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
output:
  label: ""
  aws_s3_stream:
    bucket: "" # No default (required)
    path: logs/${! timestamp_unix() }-${! uuid_v4() }.log # No default (required)
    partition_by: [] # No default (optional)
    force_path_style_urls: false
    max_buffer_bytes: 10485760
    max_buffer_count: 10000
    max_buffer_period: 10s
    content_type: application/octet-stream
    content_encoding: "" # No default (optional)
    region: ""
    endpoint: ""
    credentials:
      profile: ""
      id: ""
      secret: ""
      token: ""
      from_ec2_role: false
      role: ""
      role_external_id: ""
      expiry_window: ""
    max_in_flight: 64
    batching:
      count: 0
      byte_size: 0
      period: ""
      jitter: 0
      check: ""
      processors: [] # No default (optional)
```

</TabItem>
</Tabs>

This output writes files to S3 by streaming content incrementally using S3 multipart uploads.
Unlike the standard `aws_s3` output (which buffers the entire file in memory),
this output streams data directly to S3, reducing memory usage by 80-90% for large files.

## Key Features

- **Memory Efficient**: Streams content to S3 as buffer fills, minimal memory footprint
- **Partition Routing**: `partition_by` parameter ensures messages with same partition values go to same file
- **Dynamic Paths**: Supports Bloblang interpolation for partition paths (e.g., `logs/${! timestamp_unix() }`)
- **S3 Multipart Upload**: Leverages S3 multipart uploads for reliable large file transfers
- **Flexible Buffering**: Configure by bytes, message count, or time period

## When to Use

Use this output instead of `aws_s3` when:
- Writing large files (>1GB) that would consume too much memory
- Streaming continuous data that needs to be partitioned into files
- You need dynamic partition routing (e.g., one file per account/date combination)
- Memory-constrained environments (containers, Lambda, ECS)

## Partition Routing

The `partition_by` parameter evaluates expressions per message to determine routing.
Messages with identical partition values are written to the same file:

```yaml
partition_by:
  - '${! meta("date") }'
  - '${! meta("account") }'
```

This ensures:
- Messages routed to correct writer based on partition key
- Path (including uuid_v4()) evaluated once per partition
- Multiple concurrent writers for different partitions

### Credentials

By default Bento will use a shared credentials file when connecting to AWS services.
You can find out more [in this document](/docs/guides/cloud/aws).


## Examples

<Tabs defaultValue="Writing Partitioned Log Files" values={[
{ label: 'Writing Partitioned Log Files', value: 'Writing Partitioned Log Files', },
{ label: 'Low Memory JSON Streaming', value: 'Low Memory JSON Streaming', },
]}>

<TabItem value="Writing Partitioned Log Files">

This example writes streaming log files partitioned by date and service to S3.

```yaml
output:
  aws_s3_stream:
    bucket: my-logs-bucket
    path: 'logs/date=${! meta("date") }/service=${! meta("service") }/${! uuid_v4() }.log'

    # Messages with same date+service go to same file
    partition_by:
      - '${! meta("date") }'
      - '${! meta("service") }'

    max_buffer_bytes: 10485760  # 10MB
    max_buffer_count: 10000
    max_buffer_period: 10s
```

</TabItem>
<TabItem value="Low Memory JSON Streaming">

This example demonstrates memory-efficient streaming for large JSON datasets.

```yaml
output:
  aws_s3_stream:
    bucket: data-lake
    path: 'events/date=${! now().ts_format("2006-01-02") }/${! uuid_v4() }.json'
    content_type: application/json

    batching:
      count: 10000
      period: 10s
      processors:
        - archive:
            format: lines
```

</TabItem>
</Tabs>

## Fields

### `bucket`

The S3 bucket to upload files to.


Type: `string`  

### `path`

The path for each file. Supports Bloblang interpolation for dynamic partitioning.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `string`  

```yml
# Examples

path: logs/${! timestamp_unix() }-${! uuid_v4() }.log

path: data/date=${! meta("date") }/account=${! meta("account") }/${! uuid_v4() }.json
```

### `partition_by`

Optional list of interpolated string expressions that determine writer partitioning. Messages with the same partition values are written to the same file. The full path is only evaluated once when a new partition is encountered. This allows using functions like uuid_v4() in the path for unique filenames per partition. If omitted, the full path is evaluated per message for backwards compatibility.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `array`  

```yml
# Examples

partition_by:
  - ${! meta("date") }
  - ${! meta("account") }
```

### `force_path_style_urls`

Forces path style URLs for S3 requests.


Type: `bool`  
Default: `false`  

### `max_buffer_bytes`

Maximum buffer size in bytes before flushing to S3. Default is 10MB.


Type: `int`  
Default: `10485760`  

### `max_buffer_count`

Maximum number of messages to buffer before flushing to S3. Default is 10000.


Type: `int`  
Default: `10000`  

### `max_buffer_period`

Maximum duration to buffer messages before flushing to S3. Default is 10s.


Type: `string`  
Default: `"10s"`  

### `content_type`

The content type to set for uploaded files. Supports interpolation.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `string`  
Default: `"application/octet-stream"`  

### `content_encoding`

The content encoding to set for uploaded files (e.g., gzip). Supports interpolation.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `string`  

### `region`

The AWS region to target.


Type: `string`  
Default: `""`  

### `endpoint`

Allows you to specify a custom endpoint for the AWS API.


Type: `string`  
Default: `""`  

### `credentials`

Optional manual configuration of AWS credentials to use. More information can be found [in this document](/docs/guides/cloud/aws).


Type: `object`  

### `credentials.profile`

A profile from `~/.aws/credentials` to use.


Type: `string`  
Default: `""`  

### `credentials.id`

The ID of credentials to use.


Type: `string`  
Default: `""`  

### `credentials.secret`

The secret for the credentials being used.
:::warning Secret
This field contains sensitive information that usually shouldn't be added to a config directly, read our [secrets page for more info](/docs/configuration/secrets).
:::


Type: `string`  
Default: `""`  

### `credentials.token`

The token for the credentials being used, required when using short term credentials.


Type: `string`  
Default: `""`  

### `credentials.from_ec2_role`

Use the credentials of a host EC2 machine configured to assume [an IAM role associated with the instance](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html).


Type: `bool`  
Default: `false`  
Requires version 1.0.0 or newer  

### `credentials.role`

A role ARN to assume.


Type: `string`  
Default: `""`  

### `credentials.role_external_id`

An external ID to provide when assuming a role.


Type: `string`  
Default: `""`  

### `credentials.expiry_window`

Allow the credentials to trigger refreshing prior to the credentials actually expiring. This is beneficial so race conditions with expiring credentials do not cause requests to fail. For example '10s' would refresh credentials ten seconds before expiration. Setting to a duration of `0` disables the expiry window.


Type: `string`  
Default: `""`  

### `max_in_flight`

The maximum number of messages to have in flight at a given time. Increase this to improve throughput.


Type: `int`  
Default: `64`  

### `batching`

Allows you to configure a [batching policy](/docs/configuration/batching).


Type: `object`  

```yml
# Examples

batching:
  byte_size: 5000
  count: 0
  period: 1s

batching:
  count: 10
  period: 1s

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m

batching:
  count: 10
  jitter: 0.1
  period: 10s
```

### `batching.count`

A number of messages at which the batch should be flushed. If `0` disables count based batching.


Type: `int`  
Default: `0`  

### `batching.byte_size`

An amount of bytes at which the batch should be flushed. If `0` disables size based batching.


Type: `int`  
Default: `0`  

### `batching.period`

A period in which an incomplete batch should be flushed regardless of its size.


Type: `string`  
Default: `""`  

```yml
# Examples

period: 1s

period: 1m

period: 500ms
```

### `batching.jitter`

A non-negative factor that adds random delay to batch flush intervals, where delay is determined uniformly at random between `0` and `jitter * period`. For example, with `period: 100ms` and `jitter: 0.1`, each flush will be delayed by a random duration between `0-10ms`.


Type: `float`  
Default: `0`  

```yml
# Examples

jitter: 0.01

jitter: 0.1

jitter: 1
```

### `batching.check`

A [Bloblang query](/docs/guides/bloblang/about/) that should return a boolean value indicating whether a message should end a batch.


Type: `string`  
Default: `""`  

```yml
# Examples

check: this.type == "end_of_transaction"
```

### `batching.processors`

A list of [processors](/docs/components/processors/about) to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op.


Type: `array`  

```yml
# Examples

processors:
  - archive:
      format: concatenate

processors:
  - archive:
      format: lines

processors:
  - archive:
      format: json_array
```


