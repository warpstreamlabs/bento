---
title: aws_s3_parquet_stream
slug: aws_s3_parquet_stream
type: output
status: beta
categories: ["Services","AWS"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the corresponding source file under internal/impl/<provider>.
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

:::caution BETA
This component is mostly stable but breaking changes could still be made outside of major version releases if a fundamental problem with the component is found.
:::
Streams Parquet data directly to S3 using multipart uploads with minimal memory overhead.

Introduced in version 1.0.0.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yml
# Common config fields, showing default values
output:
  label: ""
  aws_s3_parquet_stream:
    bucket: "" # No default (required)
    path: logs/${! timestamp_unix() }-${! uuid_v4() }.parquet # No default (required)
    schema: [] # No default (optional)
    schema_file: ./schemas/ocsf_network_activity.yml # No default (optional)
    default_compression: uncompressed
    max_in_flight: 64
    batching:
      count: 0
      byte_size: 0
      period: ""
      jitter: 0
      check: ""
```

</TabItem>
<TabItem value="advanced">

```yml
# All config fields, showing default values
output:
  label: ""
  aws_s3_parquet_stream:
    bucket: "" # No default (required)
    path: logs/${! timestamp_unix() }-${! uuid_v4() }.parquet # No default (required)
    partition_by: [] # No default (optional)
    force_path_style_urls: false
    schema: [] # No default (optional)
    schema_file: ./schemas/ocsf_network_activity.yml # No default (optional)
    default_compression: uncompressed
    default_encoding: DELTA_LENGTH_BYTE_ARRAY
    row_group_size: 10000
    region: ""
    endpoint: ""
    credentials:
      profile: ""
      id: ""
      secret: ""
      token: ""
      from_ec2_role: false
      role: ""
      role_external_id: ""
      expiry_window: ""
    max_in_flight: 64
    batching:
      count: 0
      byte_size: 0
      period: ""
      jitter: 0
      check: ""
      processors: [] # No default (optional)
```

</TabItem>
</Tabs>

This output writes Parquet files to S3 by streaming row groups incrementally using S3 multipart uploads.
Unlike the standard `aws_s3` output with `parquet_encode` processor (which buffers the entire file in memory),
this output streams data directly to S3, reducing memory usage from gigabytes to under 100MB.

## Key Features

- **Memory Efficient**: Streams row groups to S3 as they're generated, minimal memory footprint
- **Automatic File Rotation**: Files are automatically closed and new ones started based on row count or duration
- **Dynamic Paths**: Supports Bloblang interpolation for partition paths (e.g., `logs/${! timestamp_unix() }`)
- **S3 Multipart Upload**: Leverages S3 multipart uploads for reliable large file transfers

## When to Use

Use this output instead of `aws_s3` + `parquet_encode` when:
- Writing large Parquet files (over 1GB) that would consume too much memory
- Streaming continuous data that needs to be partitioned into files
- You need fine-grained control over file rotation (by row count or time)

## Performance

For a typical OCSF event dataset (84,265 events):
- Standard approach: ~9GB memory usage
- Streaming approach: under 100MB memory usage

### Credentials

By default Bento will use a shared credentials file when connecting to AWS services.
You can find out more [in this document](/docs/guides/cloud/aws).


## Examples

<Tabs defaultValue="Writing Partitioned Parquet Files" values={[
{ label: 'Writing Partitioned Parquet Files', value: 'Writing Partitioned Parquet Files', },
{ label: 'Low Memory Parquet Streaming', value: 'Low Memory Parquet Streaming', },
]}>

<TabItem value="Writing Partitioned Parquet Files">

This example writes streaming Parquet files partitioned by date to S3.

```yaml
output:
  aws_s3_parquet_stream:
    bucket: my-data-bucket
    path: 'events/date=${! now().ts_format("2006-01-02") }/${! uuid_v4() }.parquet'
    schema:
      - name: id
        type: INT64
      - name: timestamp
        type: INT64
      - name: message
        type: UTF8
      - name: level
        type: UTF8
    default_compression: snappy
```

</TabItem>
<TabItem value="Low Memory Parquet Streaming">

This example demonstrates memory-efficient streaming for large datasets.

```yaml
output:
  aws_s3_parquet_stream:
    bucket: large-data-bucket
    path: 'data/${! timestamp_unix() }.parquet'
    schema:
      - name: id
        type: INT64
      - name: data
        type: BYTE_ARRAY
    default_compression: zstd
    row_group_size: 5000  # Smaller row groups = less memory
```

</TabItem>
</Tabs>

## Fields

### `bucket`

The S3 bucket to upload files to.


Type: `string`  

### `path`

The path for each Parquet file. Supports Bloblang interpolation for dynamic partitioning.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `string`  

```yml
# Examples

path: logs/${! timestamp_unix() }-${! uuid_v4() }.parquet

path: data/year=${! now().ts_format("2006") }/month=${! now().ts_format("01") }/data.parquet
```

### `partition_by`

Optional list of interpolated string expressions that determine writer partitioning. Messages with the same partition values are written to the same file. The full path is only evaluated once when a new partition is encountered. This allows using functions like uuid_v4() in the path for unique filenames per partition. If omitted, the full path is evaluated per message for backwards compatibility.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `array`  

```yml
# Examples

partition_by:
  - ${! json("event_date") }
  - ${! json("account_id") }
```

### `force_path_style_urls`

Forces path style URLs for S3 requests.


Type: `bool`  
Default: `false`  

### `schema`

Parquet schema. Mutually exclusive with schema_file.


Type: `array`  

### `schema[].name`

The name of the column.


Type: `string`  

### `schema[].type`

The type of the column, only applicable for leaf columns with no child fields. STRUCT represents nested objects with defined field schemas. MAP supports only string keys, but can support values of all types. Some logical types can be specified here such as UTF8.


Type: `string`  
Options: `BOOLEAN`, `INT8`, `INT16`, `INT32`, `INT64`, `DECIMAL64`, `DECIMAL32`, `FLOAT`, `DOUBLE`, `BYTE_ARRAY`, `UTF8`, `MAP`, `LIST`, `STRUCT`.

### `schema[].decimal_precision`

Precision to use for DECIMAL32/DECIMAL64 type


Type: `int`  
Default: `0`  

### `schema[].decimal_scale`

Scale to use for DECIMAL32/DECIMAL64 type


Type: `int`  
Default: `0`  

### `schema[].repeated`

Whether the field is repeated.


Type: `bool`  
Default: `false`  

### `schema[].optional`

Whether the field is optional.


Type: `bool`  
Default: `false`  

### `schema[].fields`

A list of child fields.


Type: `array`  

```yml
# Examples

fields:
  - name: foo
    type: INT64
  - name: bar
    type: BYTE_ARRAY
```

### `schema_file`

Path to a YAML file containing a Parquet schema definition. The file should contain a parquet_encode processor resource with a schema section. Mutually exclusive with schema.


Type: `string`  

```yml
# Examples

schema_file: ./schemas/ocsf_network_activity.yml
```

### `default_compression`

The default compression type to use for Parquet columns.


Type: `string`  
Default: `"uncompressed"`  
Options: `uncompressed`, `snappy`, `gzip`, `brotli`, `zstd`, `lz4raw`.

### `default_encoding`

The default encoding type to use for fields.


Type: `string`  
Default: `"DELTA_LENGTH_BYTE_ARRAY"`  
Options: `DELTA_LENGTH_BYTE_ARRAY`, `PLAIN`.

### `row_group_size`

Number of rows per row group. Smaller values reduce memory but increase file overhead.


Type: `int`  
Default: `10000`  

### `region`

The AWS region to target.


Type: `string`  
Default: `""`  

### `endpoint`

Allows you to specify a custom endpoint for the AWS API.


Type: `string`  
Default: `""`  

### `credentials`

Optional manual configuration of AWS credentials to use. More information can be found [in this document](/docs/guides/cloud/aws).


Type: `object`  

### `credentials.profile`

A profile from `~/.aws/credentials` to use.


Type: `string`  
Default: `""`  

### `credentials.id`

The ID of credentials to use.


Type: `string`  
Default: `""`  

### `credentials.secret`

The secret for the credentials being used.
:::warning Secret
This field contains sensitive information that usually shouldn't be added to a config directly, read our [secrets page for more info](/docs/configuration/secrets).
:::


Type: `string`  
Default: `""`  

### `credentials.token`

The token for the credentials being used, required when using short term credentials.


Type: `string`  
Default: `""`  

### `credentials.from_ec2_role`

Use the credentials of a host EC2 machine configured to assume [an IAM role associated with the instance](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html).


Type: `bool`  
Default: `false`  
Requires version 1.0.0 or newer  

### `credentials.role`

A role ARN to assume.


Type: `string`  
Default: `""`  

### `credentials.role_external_id`

An external ID to provide when assuming a role.


Type: `string`  
Default: `""`  

### `credentials.expiry_window`

Allow the credentials to trigger refreshing prior to the credentials actually expiring. This is beneficial so race conditions with expiring credentials do not cause requests to fail. For example '10s' would refresh credentials ten seconds before expiration. Setting to a duration of `0` disables the expiry window.


Type: `string`  
Default: `""`  

### `max_in_flight`

The maximum number of messages to have in flight at a given time. Increase this to improve throughput.


Type: `int`  
Default: `64`  

### `batching`

Allows you to configure a [batching policy](/docs/configuration/batching).


Type: `object`  

```yml
# Examples

batching:
  byte_size: 5000
  count: 0
  period: 1s

batching:
  count: 10
  period: 1s

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m

batching:
  count: 10
  jitter: 0.1
  period: 10s
```

### `batching.count`

A number of messages at which the batch should be flushed. If `0` disables count based batching.


Type: `int`  
Default: `0`  

### `batching.byte_size`

An amount of bytes at which the batch should be flushed. If `0` disables size based batching.


Type: `int`  
Default: `0`  

### `batching.period`

A period in which an incomplete batch should be flushed regardless of its size.


Type: `string`  
Default: `""`  

```yml
# Examples

period: 1s

period: 1m

period: 500ms
```

### `batching.jitter`

A non-negative factor that adds random delay to batch flush intervals, where delay is determined uniformly at random between `0` and `jitter * period`. For example, with `period: 100ms` and `jitter: 0.1`, each flush will be delayed by a random duration between `0-10ms`.


Type: `float`  
Default: `0`  

```yml
# Examples

jitter: 0.01

jitter: 0.1

jitter: 1
```

### `batching.check`

A [Bloblang query](/docs/guides/bloblang/about/) that should return a boolean value indicating whether a message should end a batch.


Type: `string`  
Default: `""`  

```yml
# Examples

check: this.type == "end_of_transaction"
```

### `batching.processors`

A list of [processors](/docs/components/processors/about) to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op.


Type: `array`  

```yml
# Examples

processors:
  - archive:
      format: concatenate

processors:
  - archive:
      format: lines

processors:
  - archive:
      format: json_array
```


